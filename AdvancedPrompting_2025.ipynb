{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/iwatsuki-yuuki/LLM_lecture/blob/main/AdvancedPrompting_2025.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "956d4beb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 392,
          "referenced_widgets": [
            "2616c0d2eac147eb987820dd843f3f63",
            "f4158a9fc9be43fa81607c1b1212d8c6",
            "58bf1d3fc14847168edd80d6f9c18ff5",
            "57dad4bfbe434f5fbab42efa3594b77f",
            "baa0e6e2e6404cc4825d935b34a6d8c7",
            "3d88b57876ed40c8aa424ec3bf8b2f88",
            "884ba2579625477c8dd2e61c9767efa6",
            "a5d88caeca0c498db88c9cce0cd851fa",
            "29e527bd598545508b6edcad8fd60e85",
            "b046df39f72043a1b9d04b60b540e0dc",
            "d37b71a0b1b741658656cf27e2ad7a65"
          ]
        },
        "id": "956d4beb",
        "outputId": "d59bc78d-98b0-4954-8a2f-5e91fd6bd8b8"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2616c0d2eac147eb987820dd843f3f63"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "プロンプティングは、大規模言語モデル（LLM）を小学生向けに説明することを目的としたテクニックです。この方法では、シンプルでわかりやすい言葉を使って、モデルに「どう説明するか」を指示します。\n",
            "\n",
            "例えば、「お母さんとお父さんとお子さんは一緒に遊ぶ」という文を入力すると、モデルはその文を自然な日本語で説明します。このように、プロンプティングは、モデルが複雑な内容を小学生に理解しやすいようにするための方法です。\n",
            "\n",
            "プロンプティングには、具体的な指示や、文章の構造に注意することが重要です。また、モデルはユーザーの言葉を理解し、それに基づいて回答します。そのため、説明文を明確にし、ユーザーの意図を正確に把握することが大切です。\n",
            "\n",
            "このテクニックは、小学生向けの教材や学習支援にとても有用です。モデルが理解しやすい言葉を使って、説明をすることで、学習効果が向上します。\n",
            "-----\n",
            "プロンプティングは、大規模言語モデル（LLM）に特定のタスクや要求を伝えるための技術です。小学生向けに説明する場合、シンプルでわかりやすい言葉を使って説明することが重要です。\n",
            "\n",
            "プロンプティングの基本は、「どんなことを求めているか」を明確に伝えることです。例えば、「この問題を解け」というと、モデルは問題文を理解し、答えを出します。しかし、より具体的な指示を出すことで、モデルはより正確に答えを出します。\n",
            "\n",
            "例えば、「5＋3＝？」と聞かれた場合、モデルは答えを出すことができます。しかし、「5＋3はいくつですか？」と聞かれた場合、答えを出すだけでなく、その計算方法も説明するよう求められます。\n",
            "\n",
            "また、プロンプティングでは、言語の流れや文章の構造を意識するのも重要です。例えば、「この文章の意味を説明してください」と聞かれた場合、モデルは文章の内容を理解し、説明をします。\n",
            "\n",
            "小学生向けに、プロンプティングは「どんなことを聞いているか」を明確に伝えることで、より良い答えを提供できるようにします。このようにして、言語モデルは、より自然で理解しやすい答えを出すことができます。\n",
            "-----\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import torch\n",
        "\n",
        "model_name = \"Qwen/Qwen3-1.7B\"\n",
        "\n",
        "# load the tokenizer and the model\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    torch_dtype=\"auto\",\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "# prepare the model input\n",
        "prompt = \"小学生向けに説明することを想定して、大規模言語モデルを活用するテクニックであるプロンプティングについて説明してください。600文字程度でMarkdownは使用せず、説明文のみ出力してください。\"\n",
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": prompt}\n",
        "]\n",
        "text = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    tokenize=False,\n",
        "    add_generation_prompt=True,\n",
        "    enable_thinking=False,\n",
        ")\n",
        "model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "# conduct text completion\n",
        "with torch.no_grad():\n",
        "    generated_ids = model.generate(\n",
        "        **model_inputs,\n",
        "        max_new_tokens=1024,\n",
        "        temperature=1.0,\n",
        "        top_p=0.7,\n",
        "        num_return_sequences=2,\n",
        "    )\n",
        "for sequence in generated_ids:\n",
        "    print(tokenizer.decode(sequence[len(model_inputs.input_ids[0]):], skip_special_tokens=True))\n",
        "    print(\"-----\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fb503173",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fb503173",
        "outputId": "f643b547-2ca0-42a8-cc7a-4895c0f50d10"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<評価の理由>\n",
            "文章はプロンプティングの意味を分かりやすく説明し、小学生向けに親しみやすい言葉で説明しています。具体的な例を挙げて説明し、親や先生に教えてあげると良いと述べています。内容は適切で、説明が丁寧です。\n",
            "\n",
            "<点数>\n",
            "90\n"
          ]
        }
      ],
      "source": [
        "judge_prompt = \"\"\"<評価対象の文章>がプロンティングについて、小学生向けにどれほど適切に説明しているか、100点満点で評価してください。\n",
        "評価に際しては以下の基準を参考にしてください。\n",
        "- プロンプティングとはAIへの指示の出し方であることを説明していること\n",
        "- 説明の最初から小学生に語りかけるような優しい口調で説明していること\n",
        "- 小学生に身近な具体例を交えて、詳しく指示出しを行うほどAIの応答が良くなることを説明していること\n",
        "\n",
        "<評価対象の文章>\n",
        "{content}\n",
        "</評価対象の文章>\n",
        "\n",
        "出力は以下のフォーマットに従ってください。\n",
        "<評価の理由>\n",
        "評価の理由を200文字程度で記載してください\n",
        "</評価の理由>\n",
        "\n",
        "<点数>\n",
        "点数を100点満点で記載してください\n",
        "</点数>\"\"\"\n",
        "\n",
        "content = \"\"\"プロンプティングは、大規模言語モデル（LLM）に指示を伝えるための方法であり、小学生向けに説明するときには分かりやすく、親しみやすい言葉で説明することが重要です。\n",
        "\n",
        "プロンプティングとは、モデルに「何をしたいか」を具体的に伝え、モデルがそれに応じて答えを返す仕組みのことです。例えば、「この文章の続きを書け」や「100字以内でこの物語をまとめなさい」といったように、モデルに「何を求めるか」を明確に伝えます。\n",
        "\n",
        "小学生向けには、簡単な言葉で説明し、具体的な例を挙げてわかりやすく説明するのが効果的です。また、モデルに「どんな答えをほしいか」を伝えることで、より良い回答が得られます。\n",
        "\n",
        "プロンプティングは、単純な指示だけでなく、複雑な問題も解決するための力です。小学生でも理解しやすく、使いこなせるよう、親や先生に教えてあげると良いでしょう。\"\"\"\n",
        "\n",
        "judge_messages = [\n",
        "    {\"role\": \"user\", \"content\": judge_prompt.format(content=content)}\n",
        "]\n",
        "judge_text = tokenizer.apply_chat_template(\n",
        "    judge_messages,\n",
        "    tokenize=False,\n",
        "    add_generation_prompt=True,\n",
        "    enable_thinking=False,\n",
        ")\n",
        "judge_inputs = tokenizer([judge_text], return_tensors=\"pt\").to(model.device)\n",
        "with torch.no_grad():\n",
        "    judge_ids = model.generate(\n",
        "        **judge_inputs,\n",
        "        max_new_tokens=4096,\n",
        "        do_sample=False,\n",
        "    )\n",
        "judge_output = tokenizer.decode(judge_ids[0][len(judge_inputs.input_ids[0]):], skip_special_tokens=True)\n",
        "print(judge_output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2ea3ce9e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ea3ce9e",
        "outputId": "9beac842-deac-4705-9287-09d4469f2ff8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<評価の理由>\n",
            "プロンプティングとはAIに指示を伝える方法であり、小学生向けには分かりやすく具体的な言葉で説明している。説明の内容が親しみやすいと感じられる。具体例も少なく、効果についても少し浅い説明だが、基本的な説明はされている。\n",
            "</評価の理由>\n",
            "\n",
            "<点数>\n",
            "70\n",
            "</点数>\n"
          ]
        }
      ],
      "source": [
        "judge_prompt_one_shot = \"\"\"<評価対象の文章>がプロンティングについて、小学生向けにどれほど適切に説明しているか、100点満点で評価してください。\n",
        "評価に際しては以下の加点基準を参考にしてください。\n",
        "- プロンプティングとはAIへの指示の出し方であることを説明していること\n",
        "- 説明の最初から小学生に語りかけるような優しい口調で説明していること\n",
        "- 小学生に身近な具体例を交えて、詳しく指示出しを行うほどAIの応答が良くなることを説明していること\n",
        "\n",
        "出力は以下のフォーマットに従ってください。\n",
        "<評価の理由>\n",
        "評価の理由を200文字程度で記載してください\n",
        "</評価の理由>\n",
        "\n",
        "<点数>\n",
        "点数を100点満点で記載してください\n",
        "</点数>\n",
        "\n",
        "評価の例\n",
        "<評価対象の文章>\n",
        "プロンプティングは、大規模言語モデルを小学生向けに説明するためのテクニックです。この方法では、モデルに「どう説明するか」を具体的に指示することで、複雑な概念を簡単に理解できるようにします。\n",
        "\n",
        "例えば、「お母さんとお父さん」をテーマにした説明を求めるとき、モデルは「お母さんとお父さんとは、どう違うのか？」と聞かれることがあります。このとき、モデルは具体的な例やイメージを用いて、お母さんとお父さんを比較し、違いを分かりやすく説明します。\n",
        "\n",
        "プロンプティングでは、説明の内容を「どう説明するか」に焦点を当てて指示します。このようにすることで、モデルは適切な言葉や表現を使って、小学生にも理解しやすい説明を提供できます。\n",
        "\n",
        "また、プロンプティングは、モデルの理解力や表現力にも影響を与えます。具体的な指示をすることで、モデルはより良い説明を提供できるようになります。したがって、プロンプティングは、小学生向けに言語モデルを活用する上で非常に重要なテクニックです。\n",
        "</評価対象の文章>\n",
        "\n",
        "<評価の理由>\n",
        "「プロンプティングは、大規模言語モデルを小学生向けに説明するためのテクニック」は事実として誤り。\n",
        "説明の最初から語りかけているような優しい口調ではない。\n",
        "具体例も不十分で、プロンプティングの効果についても浅い説明。\n",
        "</評価の理由>\n",
        "\n",
        "<点数>\n",
        "50\n",
        "</点数>\n",
        "\n",
        "<評価対象の文章>\n",
        "{content}\n",
        "</評価対象の文章>\n",
        "\"\"\"\n",
        "\n",
        "content = \"\"\"プロンプティングは、大規模言語モデル（LLM）に指示を伝えるための方法であり、小学生向けに説明するときには分かりやすく、親しみやすい言葉で説明することが重要です。\n",
        "\n",
        "プロンプティングとは、モデルに「何をしたいか」を具体的に伝え、モデルがそれに応じて答えを返す仕組みのことです。例えば、「この文章の続きを書け」や「100字以内でこの物語をまとめなさい」といったように、モデルに「何を求めるか」を明確に伝えます。\n",
        "\n",
        "小学生向けには、簡単な言葉で説明し、具体的な例を挙げてわかりやすく説明するのが効果的です。また、モデルに「どんな答えをほしいか」を伝えることで、より良い回答が得られます。\n",
        "\n",
        "プロンプティングは、単純な指示だけでなく、複雑な問題も解決するための力です。小学生でも理解しやすく、使いこなせるよう、親や先生に教えてあげると良いでしょう。\"\"\"\n",
        "\n",
        "judge_messages = [\n",
        "    {\"role\": \"user\", \"content\": judge_prompt_one_shot.format(content=content)}\n",
        "]\n",
        "judge_text = tokenizer.apply_chat_template(\n",
        "    judge_messages,\n",
        "    tokenize=False,\n",
        "    add_generation_prompt=True,\n",
        "    enable_thinking=False,\n",
        ")\n",
        "judge_inputs = tokenizer([judge_text], return_tensors=\"pt\").to(model.device)\n",
        "with torch.no_grad():\n",
        "    judge_ids = model.generate(\n",
        "        **judge_inputs,\n",
        "        max_new_tokens=4096,\n",
        "        do_sample=False,\n",
        "    )\n",
        "judge_output = tokenizer.decode(judge_ids[0][len(judge_inputs.input_ids[0]):], skip_special_tokens=True)\n",
        "print(judge_output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4f35ffe2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4f35ffe2",
        "outputId": "6bd2f353-bbc0-4f2d-930a-8982377467bd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<評価の理由>\n",
            "プロンプティングとはAIに指示を伝える方法であり、小学生向けには分かりやすく具体的な言葉で説明している。説明の内容が親しみやすいと感じられる。具体例も少なく、効果についても少し浅い説明だが、基本的な説明はされている。\n",
            "</評価の理由>\n",
            "\n",
            "<点数>\n",
            "70\n",
            "</点数>\n"
          ]
        }
      ],
      "source": [
        "print(judge_output)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a29d8c59",
      "metadata": {
        "id": "a29d8c59"
      },
      "source": [
        "LLMの出力を効果的に抜き出したい時はstructured outputを活用するのがおすすめです。\n",
        "\n",
        "https://platform.openai.com/docs/guides/structured-outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6c4ef058",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6c4ef058",
        "outputId": "091409f8-0fae-4b5e-908c-b0b64d238d13"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score: 70\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "def parse_score(output: str) -> int:\n",
        "    pattern = r\"<点数>\\s*(\\d{1,3})\\s*\"\n",
        "    match = re.search(pattern, output)\n",
        "    if match:\n",
        "        return int(match.group(1))\n",
        "    else:\n",
        "        return -1\n",
        "score = parse_score(judge_output)\n",
        "print(f\"Score: {score}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "930ef60f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "930ef60f",
        "outputId": "779dc07e-b25c-43f5-9d0a-ff38a4527073"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "New best score: 50\n",
            "おもちゃや遊具を一緒に遊ぶように、AIも一緒に考えてくれるよ！  \n",
            "プロンプティングとは、AIに「どうしたらいいか」を伝える方法のこと。  \n",
            "例えば、「お母さん、お手伝いをしましょう」というように、自分がどうしたいかを伝えるのと同じように、AIに「どうしたら答えがほしいか」を伝えるのをプロンプティングと呼ぶんだ。\n",
            "\n",
            "小学生向けに考えると、お手伝いをしようというように、AIに「答えをほしい」という気持ちを伝えることが大切。  \n",
            "例えば、「お母さん、お手伝いをしましょう」というように、自分の気持ちを伝えると、AIもそれに応じて答えをくれるよ。\n",
            "\n",
            "具体的には、「お母さん、お手伝いをしましょう」というように、自分の気持ちを伝えると、AIもそれに応じて答えをくれるよ。  \n",
            "このように、自分の気持ちを伝えることで、AIもしっかり考えて答えをくれるから、お手伝いがうまくいくよ！\n",
            "New best score: 80\n",
            "プロンプティングは、AIに「どうすればいいか」を教えてあげる仕組みです。簡単に言うと、AIに「何をしたいか」を具体的に説明してあげることです。\n",
            "\n",
            "例えば、「猫の写真を描いてほしい」というと、AIはただの写真を描くだけですが、プロンプティングでは「猫が白い、大きな目をしている」というように、具体的な情報を伝えれば、AIはそれに合わせて良い写真を作ってくれます。\n",
            "\n",
            "小学生向けに言うと、例えば「猫が白い、大きな目をしている」というように、簡単にでも伝えれば、AIはそれに従って良い写真を描いてくれます。\n",
            "\n",
            "プロンプティングは、AIが正しい答えを出すために、どんな情報を伝えればいいかを教えてあげる仕組みです。だから、具体的で分かりやすい指示を伝えることが大切です。そうすることで、AIの応答も良くなりますよ。\n",
            "New best score: 85\n",
            "プロンプティングは、AIに「どうすればいいか」を伝える方法です。例えば、「おうちの犬が吠えることがあります。どうしたら吠えなくなるか」って聞かれたとき、AIはその問いに答えます。\n",
            "\n",
            "小学生向けに考えると、例えば「おうちの猫がおにぎりを食べたいとおもうとき、どうしたらいいの？」と聞かれた場合、AIは「おにぎりを手にとればいい」と答えます。\n",
            "\n",
            "プロンプティングは、AIが思い思いに答えを出すのではなく、私たちの指示に沿って答えを出すようにする方法です。例えば、「おうちの猫がおにぎりを食べたいとおもうとき、どうしたらいいの？」と聞かれたとき、AIは「おにぎりを手にとればいい」と答えます。\n",
            "\n",
            "具体的な例で説明すると、「おうちの犬が吠えるとき、どうしたらいいの？」と聞かれたとき、AIは「犬を静かにさせたり、おにぎりを食べさせたりする」と答えます。\n",
            "\n",
            "プロンプティングは、AIが私たちの指示に沿って答えを出すようにする方法です。しっかり指示を出すことで、AIの応答も良いものになります。\n"
          ]
        }
      ],
      "source": [
        "# Best-of-N\n",
        "prompt = \"\"\"小学生向けに説明することを想定して、大規模言語モデルを活用するテクニックであるプロンプティングについて説明してください。600文字程度でMarkdownは使用せず、説明文のみ出力してください。説明文を作成する際には以下の点を考慮してください。\n",
        "- プロンプティングとはAIへの指示の出し方であることを説明していること\n",
        "- 説明の最初から小学生に語りかけるような優しい口調で説明していること\n",
        "- 小学生に身近な具体例を交えて、詳しく指示出しを行うほどAIの応答が良くなることを説明していること\"\"\"\n",
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": prompt}\n",
        "]\n",
        "text = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    tokenize=False,\n",
        "    add_generation_prompt=True,\n",
        "    enable_thinking=False,\n",
        ")\n",
        "model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "# conduct text completion\n",
        "with torch.no_grad():\n",
        "    generated_ids = model.generate(\n",
        "        **model_inputs,\n",
        "        max_new_tokens=1024,\n",
        "        temperature=1.0,\n",
        "        top_p=0.7,\n",
        "        num_return_sequences=4,\n",
        "    )\n",
        "\n",
        "max_score = -1\n",
        "best_output = \"\"\n",
        "for sequence in generated_ids:\n",
        "    candidate = tokenizer.decode(sequence[len(model_inputs.input_ids[0]):], skip_special_tokens=True)\n",
        "\n",
        "    judge_messages = [\n",
        "        {\"role\": \"user\", \"content\": judge_prompt_one_shot.format(content=candidate)}\n",
        "    ]\n",
        "    judge_text = tokenizer.apply_chat_template(\n",
        "        judge_messages,\n",
        "        tokenize=False,\n",
        "        add_generation_prompt=True,\n",
        "        enable_thinking=False,\n",
        "    )\n",
        "    judge_inputs = tokenizer([judge_text], return_tensors=\"pt\").to(model.device)\n",
        "    with torch.no_grad():\n",
        "        judge_ids = model.generate(\n",
        "            **judge_inputs,\n",
        "            max_new_tokens=4096,\n",
        "            do_sample=False,\n",
        "        )\n",
        "    judge_output = tokenizer.decode(judge_ids[0][len(judge_inputs.input_ids[0]):], skip_special_tokens=True)\n",
        "    score = parse_score(judge_output)\n",
        "    if score == -1:\n",
        "        print(\"Failed to parse score.\")\n",
        "        print(candidate)\n",
        "    if score > max_score:\n",
        "        max_score = score\n",
        "        best_output = candidate\n",
        "        print(f\"New best score: {max_score}\")\n",
        "        print(best_output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c74e5b6a",
      "metadata": {
        "id": "c74e5b6a"
      },
      "outputs": [],
      "source": [
        "# from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "# import torch\n",
        "\n",
        "# model_name = \"Qwen/Qwen3-4B\"\n",
        "\n",
        "# # load the tokenizer and the model\n",
        "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "# model = AutoModelForCausalLM.from_pretrained(\n",
        "#     model_name,\n",
        "#     torch_dtype=\"auto\",\n",
        "#     device_map=\"auto\"\n",
        "# )\n",
        "\n",
        "# judge_prompt_one_shot = \"\"\"<評価対象の文章>がプロンティングについて、小学生向けにどれほど適切に説明しているか、100点満点で評価してください。\n",
        "# 評価に際しては以下の加点基準を参考にしてください。\n",
        "# - プロンプティングとはAIへの指示の出し方であることを説明していること\n",
        "# - 説明の最初から小学生に語りかけるような優しい口調で説明していること\n",
        "# - 小学生に身近な具体例を交えて、詳しく指示出しを行うほどAIの応答が良くなることを説明していること\n",
        "\n",
        "# 出力は以下のフォーマットに従ってください。\n",
        "# <評価の理由>\n",
        "# 評価の理由を200文字程度で記載してください\n",
        "# </評価の理由>\n",
        "\n",
        "# <点数>\n",
        "# 点数を100点満点で記載してください\n",
        "# </点数>\n",
        "\n",
        "# 評価の例\n",
        "# <評価対象の文章>\n",
        "# プロンプティングは、大規模言語モデルを小学生向けに説明するためのテクニックです。この方法では、モデルに「どう説明するか」を具体的に指示することで、複雑な概念を簡単に理解できるようにします。\n",
        "\n",
        "# 例えば、「お母さんとお父さん」をテーマにした説明を求めるとき、モデルは「お母さんとお父さんとは、どう違うのか？」と聞かれることがあります。このとき、モデルは具体的な例やイメージを用いて、お母さんとお父さんを比較し、違いを分かりやすく説明します。\n",
        "\n",
        "# プロンプティングでは、説明の内容を「どう説明するか」に焦点を当てて指示します。このようにすることで、モデルは適切な言葉や表現を使って、小学生にも理解しやすい説明を提供できます。\n",
        "\n",
        "# また、プロンプティングは、モデルの理解力や表現力にも影響を与えます。具体的な指示をすることで、モデルはより良い説明を提供できるようになります。したがって、プロンプティングは、小学生向けに言語モデルを活用する上で非常に重要なテクニックです。\n",
        "# </評価対象の文章>\n",
        "\n",
        "# <評価の理由>\n",
        "# 「プロンプティングは、大規模言語モデルを小学生向けに説明するためのテクニック」は事実として誤り。\n",
        "# 説明の最初から語りかけているような優しい口調ではない。\n",
        "# 具体例も不十分で、プロンプティングの効果についても浅い説明。\n",
        "# </評価の理由>\n",
        "\n",
        "# <点数>\n",
        "# 50\n",
        "# </点数>\n",
        "\n",
        "# <評価対象の文章>\n",
        "# {content}\n",
        "# </評価対象の文章>\n",
        "# \"\"\"\n",
        "\n",
        "# import re\n",
        "# def parse_score(output: str) -> int:\n",
        "#     pattern = r\"<点数>\\s*(\\d{1,3})\\s*\"\n",
        "#     match = re.search(pattern, output)\n",
        "#     if match:\n",
        "#         return int(match.group(1))\n",
        "#     else:\n",
        "#         return -1\n",
        "\n",
        "# # Best-of-N\n",
        "# prompt = \"\"\"小学生向けに説明することを想定して、大規模言語モデルを活用するテクニックであるプロンプティングについて説明してください。600文字程度でMarkdownは使用せず、説明文のみ出力してください。説明文を作成する際には以下の点を考慮してください。\n",
        "# - プロンプティングとはAIへの指示の出し方であることを説明していること\n",
        "# - 説明の最初から小学生に語りかけるような優しい口調で説明していること\n",
        "# - 小学生に身近な具体例を交えて、詳しく指示出しを行うほどAIの応答が良くなることを説明していること\"\"\"\n",
        "# messages = [\n",
        "#     {\"role\": \"user\", \"content\": prompt}\n",
        "# ]\n",
        "# text = tokenizer.apply_chat_template(\n",
        "#     messages,\n",
        "#     tokenize=False,\n",
        "#     add_generation_prompt=True,\n",
        "#     enable_thinking=False,\n",
        "# )\n",
        "# model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "# # conduct text completion\n",
        "# with torch.no_grad():\n",
        "#     generated_ids = model.generate(\n",
        "#         **model_inputs,\n",
        "#         max_new_tokens=1024,\n",
        "#         temperature=1.0,\n",
        "#         top_p=0.7,\n",
        "#         num_return_sequences=8,\n",
        "#     )\n",
        "\n",
        "# max_score = -1\n",
        "# best_output = \"\"\n",
        "# for sequence in generated_ids:\n",
        "#     candidate = tokenizer.decode(sequence[len(model_inputs.input_ids[0]):], skip_special_tokens=True)\n",
        "\n",
        "#     judge_messages = [\n",
        "#         {\"role\": \"user\", \"content\": judge_prompt_one_shot.format(content=candidate)}\n",
        "#     ]\n",
        "#     judge_text = tokenizer.apply_chat_template(\n",
        "#         judge_messages,\n",
        "#         tokenize=False,\n",
        "#         add_generation_prompt=True,\n",
        "#         enable_thinking=False,\n",
        "#     )\n",
        "#     judge_inputs = tokenizer([judge_text], return_tensors=\"pt\").to(model.device)\n",
        "#     with torch.no_grad():\n",
        "#         judge_ids = model.generate(\n",
        "#             **judge_inputs,\n",
        "#             max_new_tokens=4096,\n",
        "#             do_sample=False,\n",
        "#         )\n",
        "#     judge_output = tokenizer.decode(judge_ids[0][len(judge_inputs.input_ids[0]):], skip_special_tokens=True)\n",
        "#     score = parse_score(judge_output)\n",
        "#     print(score)\n",
        "#     if score == -1:\n",
        "#         print(\"Failed to parse score.\")\n",
        "#         print(candidate)\n",
        "#     if score > max_score:\n",
        "#         max_score = score\n",
        "#         best_output = candidate\n",
        "#         print(f\"New best score: {max_score}\")\n",
        "#         print(best_output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4c865633",
      "metadata": {
        "id": "4c865633"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "2616c0d2eac147eb987820dd843f3f63": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f4158a9fc9be43fa81607c1b1212d8c6",
              "IPY_MODEL_58bf1d3fc14847168edd80d6f9c18ff5",
              "IPY_MODEL_57dad4bfbe434f5fbab42efa3594b77f"
            ],
            "layout": "IPY_MODEL_baa0e6e2e6404cc4825d935b34a6d8c7"
          }
        },
        "f4158a9fc9be43fa81607c1b1212d8c6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3d88b57876ed40c8aa424ec3bf8b2f88",
            "placeholder": "​",
            "style": "IPY_MODEL_884ba2579625477c8dd2e61c9767efa6",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "58bf1d3fc14847168edd80d6f9c18ff5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a5d88caeca0c498db88c9cce0cd851fa",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_29e527bd598545508b6edcad8fd60e85",
            "value": 2
          }
        },
        "57dad4bfbe434f5fbab42efa3594b77f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b046df39f72043a1b9d04b60b540e0dc",
            "placeholder": "​",
            "style": "IPY_MODEL_d37b71a0b1b741658656cf27e2ad7a65",
            "value": " 2/2 [00:01&lt;00:00,  1.56it/s]"
          }
        },
        "baa0e6e2e6404cc4825d935b34a6d8c7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3d88b57876ed40c8aa424ec3bf8b2f88": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "884ba2579625477c8dd2e61c9767efa6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a5d88caeca0c498db88c9cce0cd851fa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "29e527bd598545508b6edcad8fd60e85": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b046df39f72043a1b9d04b60b540e0dc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d37b71a0b1b741658656cf27e2ad7a65": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}